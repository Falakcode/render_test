#!/usr/bin/env python3
"""
================================================================================
üèõÔ∏è LONDON STRATEGIC EDGE - COMPLETE BULLETPROOF WORKER v3.0
================================================================================

ARCHITECTURE:
‚îú‚îÄ‚îÄ 1Ô∏è‚É£ Tick Streaming (202 symbols)
‚îÇ   ‚îú‚îÄ‚îÄ WebSocket (~170ms latency) - 114 symbols
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Crypto: 44 pairs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Forex: 66 pairs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Commodities: 4 pairs
‚îÇ   ‚îî‚îÄ‚îÄ REST Polling (60s intervals) - 88 symbols
‚îÇ       ‚îî‚îÄ‚îÄ US Stocks: 88 symbols (market hours only)
‚îÇ
‚îú‚îÄ‚îÄ 2Ô∏è‚É£ Economic Calendar (hourly)
‚îÇ   ‚îî‚îÄ‚îÄ Trading Economics scraper
‚îÇ
‚îú‚îÄ‚îÄ 3Ô∏è‚É£ Financial News (smart scheduling)
‚îÇ   ‚îî‚îÄ‚îÄ 13 RSS feeds + DeepSeek classification + cost optimization
‚îÇ
‚îú‚îÄ‚îÄ 4Ô∏è‚É£ Gap Detection & Backfill (every 5 min)
‚îÇ   ‚îî‚îÄ‚îÄ Auto-healing for missed candles
‚îÇ
‚îú‚îÄ‚îÄ 5Ô∏è‚É£ FRED Macro Data (hourly)
‚îÇ   ‚îî‚îÄ‚îÄ US economic indicators
‚îÇ
‚îî‚îÄ‚îÄ 6Ô∏è‚É£ Health Server (port 10000)
    ‚îî‚îÄ‚îÄ Keeps Render worker alive

REQUIREMENTS:
- TwelveData Pro 610 plan
- Supabase database
- DeepSeek API (optional, for news)
- FRED API (optional, for macro data)

================================================================================
"""

import os
import sys
import json
import asyncio
import logging
import signal
from datetime import datetime, timezone, timedelta
from decimal import Decimal, InvalidOperation
from difflib import SequenceMatcher
from typing import Optional, List, Dict, Set, Any
from collections import defaultdict
from contextlib import suppress
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading

import websockets
import httpx
import requests
from bs4 import BeautifulSoup
from supabase import create_client
import feedparser

# Optional: OpenAI-compatible client for DeepSeek
try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================

TD_API_KEY = os.environ.get("TWELVEDATA_API_KEY", "77a8bbac4826469783d82ba94ee38ba7")
SUPABASE_URL = os.environ.get("SUPABASE_URL", "https://bivgdpiibqruqztnrciz.supabase.co")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
FRED_API_KEY = os.environ.get("FRED_API_KEY")
PORT = int(os.environ.get("PORT", 10000))

# Table names
TICK_TABLE = os.environ.get("SUPABASE_TABLE", "tickdata")

# ============================================================================
# SYMBOL DEFINITIONS - 202 TOTAL
# ============================================================================

# CRYPTO - 44 symbols (WebSocket)
CRYPTO_SYMBOLS = [
    # Major pairs
    "BTC/USD", "ETH/USD", "XRP/USD", "SOL/USD", "ADA/USD", "DOGE/USD",
    "BNB/USD", "XMR/USD", "AVAX/USD", "LINK/USD", "DOT/USD", "LTC/USD",
    "ATOM/USD", "UNI/USD", "XLM/USD", "ALGO/USD", "VET/USD", "FIL/USD",
    "NEAR/USD", "APE/USD", "MANA/USD", "SAND/USD", "AAVE/USD", "GRT/USD",
    "LDO/USD", "OP/USD", "ARB/USD", "CRV/USD", "SNX/USD", "COMP/USD",
    "SUSHI/USD", "YFI/USD", "1INCH/USD", "ETC/USD", "BCH/USD", "TRX/USD",
    "SHIB/USD",
    # BTC pairs
    "ETH/BTC", "XRP/BTC", "SOL/BTC", "ADA/BTC", "DOGE/BTC", "LINK/BTC", "AVAX/BTC"
]

# FOREX - 66 symbols (WebSocket)
FOREX_SYMBOLS = [
    # Majors
    "EUR/USD", "GBP/USD", "USD/JPY", "USD/CHF", "AUD/USD", "USD/CAD", "NZD/USD",
    # Crosses
    "EUR/GBP", "EUR/JPY", "GBP/JPY", "EUR/CHF", "AUD/JPY", "GBP/CHF", "EUR/AUD",
    "EUR/CAD", "AUD/CAD", "NZD/CAD", "GBP/AUD", "GBP/CAD", "CHF/JPY", "NZD/JPY",
    "AUD/NZD", "EUR/NZD", "GBP/NZD", "CAD/CHF", "CAD/JPY", "AUD/CHF", "NZD/CHF",
    # Exotics - Americas
    "USD/MXN", "USD/BRL", "USD/CLP", "USD/COP", "USD/ARS",
    # Exotics - Europe
    "USD/TRY", "USD/PLN", "USD/SEK", "USD/NOK", "USD/DKK", "USD/CZK", "USD/HUF",
    "EUR/TRY", "EUR/SEK", "EUR/NOK", "EUR/PLN", "EUR/HUF", "EUR/CZK",
    "GBP/TRY", "GBP/PLN", "GBP/SEK", "GBP/NOK", "GBP/ZAR",
    # Exotics - Africa
    "USD/ZAR",
    # Exotics - Asia Pacific
    "USD/HKD", "USD/SGD", "USD/THB", "USD/CNH", "USD/PHP", "USD/IDR",
    "USD/INR", "USD/KRW", "USD/TWD",
    "AUD/SGD", "NZD/SGD", "SGD/JPY", "HKD/JPY"
]

# COMMODITIES - 4 symbols (WebSocket)
COMMODITY_SYMBOLS = [
    "XAU/USD",  # Gold
    "XAG/USD",  # Silver
    "XPT/USD",  # Platinum
    "XPD/USD",  # Palladium
]

# US STOCKS - 88 symbols (REST API polling)
STOCK_SYMBOLS = [
    # Mega Cap Tech (8)
    "AAPL", "MSFT", "GOOGL", "GOOG", "AMZN", "NVDA", "META", "TSLA",
    # Tech Leaders (22)
    "AMD", "INTC", "CRM", "ORCL", "ADBE", "NOW", "SNOW", "PLTR", "NET", "DDOG",
    "MDB", "SHOP", "SQ", "PYPL", "COIN", "MSTR", "UBER", "ABNB", "DASH", "LYFT",
    "ZM", "ROKU",
    # Semiconductors (10)
    "AVGO", "QCOM", "TXN", "MU", "LRCX", "AMAT", "KLAC", "ASML", "TSM", "ARM",
    # Finance (11)
    "JPM", "BAC", "WFC", "GS", "MS", "C", "BLK", "SCHW", "AXP", "V", "MA",
    # Healthcare (10)
    "JNJ", "UNH", "PFE", "ABBV", "MRK", "LLY", "TMO", "ABT", "BMY", "AMGN",
    # Consumer (10)
    "WMT", "HD", "COST", "NKE", "SBUX", "MCD", "KO", "PEP", "PG", "DIS",
    # Energy (5)
    "XOM", "CVX", "COP", "SLB", "OXY",
    # Industrial (5)
    "CAT", "DE", "BA", "GE", "HON",
    # ETFs (7)
    "SPY", "QQQ", "IWM", "DIA", "VTI", "VOO", "XLF",
]

# Combined WebSocket symbols
WS_SYMBOLS = CRYPTO_SYMBOLS + FOREX_SYMBOLS + COMMODITY_SYMBOLS
WS_SYMBOLS_STR = ",".join(WS_SYMBOLS)

# All symbols for gap detection (original 16 from your current setup)
CANDLE_SYMBOLS = [
    "BTC/USD", "ETH/USD", "XRP/USD", "XMR/USD", "SOL/USD", "BNB/USD", "ADA/USD", "DOGE/USD",
    "XAU/USD", "EUR/USD", "GBP/USD", "USD/CAD", "GBP/AUD", "AUD/CAD", "EUR/GBP", "USD/JPY"
]

# ============================================================================
# TIMING CONSTANTS
# ============================================================================

# Tick streaming
BATCH_MAX = 500
BATCH_FLUSH_SECS = 2
WATCHDOG_TIMEOUT_SECS = 30
CONNECTION_TIMEOUT_SECS = 15
MAX_RECONNECT_DELAY = 30
HEALTH_LOG_INTERVAL = 60

# Stock polling
STOCK_POLL_INTERVAL = 60

# Gap detection
GAP_SCAN_INTERVAL = 300      # 5 minutes
MAX_GAP_HOURS = 24
MIN_GAP_MINUTES = 2
BACKFILL_RATE_LIMIT = 1.2

# Economic calendar
ECON_SCRAPE_INTERVAL = 3600  # 1 hour

# FRED API
FRED_API_BASE = "https://api.stlouisfed.org/fred"

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
log = logging.getLogger("LSE")

# Reduce noise
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("websockets").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

# ============================================================================
# DATABASE CLIENTS
# ============================================================================

sb = None
if SUPABASE_KEY:
    try:
        sb = create_client(SUPABASE_URL, SUPABASE_KEY)
        log.info("‚úÖ Supabase client initialized")
    except Exception as e:
        log.error(f"‚ùå Supabase init failed: {e}")
else:
    log.warning("‚ö†Ô∏è SUPABASE_SERVICE_ROLE_KEY not set")

deepseek_client = None
if DEEPSEEK_API_KEY and HAS_OPENAI:
    try:
        deepseek_client = openai.OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        log.info("‚úÖ DeepSeek client initialized")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è DeepSeek init failed: {e}")

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def to_decimal(value: Any) -> Optional[Decimal]:
    """Safely convert to Decimal with full precision"""
    if value is None:
        return None
    try:
        return Decimal(str(value))
    except (InvalidOperation, ValueError):
        return None


def to_float(value: Any) -> Optional[float]:
    """Safely convert to float"""
    if value is None:
        return None
    try:
        return float(Decimal(str(value)))
    except (InvalidOperation, ValueError, TypeError):
        return None


def to_timestamp(value: Any) -> datetime:
    """Convert various formats to UTC datetime"""
    if value is None:
        return datetime.now(timezone.utc)

    try:
        ts = float(value)
        if ts > 1e12:
            ts /= 1000
        return datetime.fromtimestamp(ts, tz=timezone.utc)
    except (ValueError, TypeError, OSError):
        pass

    try:
        dt = datetime.fromisoformat(str(value).replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except ValueError:
        pass

    return datetime.now(timezone.utc)


def is_us_market_open() -> bool:
    """Check if US stock market is open (9:30 AM - 4:00 PM ET)"""
    now = datetime.now(timezone.utc)
    
    # Weekend check
    if now.weekday() >= 5:
        return False
    
    # Convert to ET (simplified: UTC-5)
    et_hour = (now.hour - 5) % 24
    et_minute = now.minute
    
    # Market hours: 9:30 AM - 4:00 PM ET
    if et_hour == 9 and et_minute >= 30:
        return True
    if 10 <= et_hour < 16:
        return True
    
    return False


def is_forex_market_open() -> bool:
    """Check if forex market is open"""
    now = datetime.now(timezone.utc)
    weekday = now.weekday()
    hour = now.hour
    
    if weekday == 5:  # Saturday
        return False
    if weekday == 6 and hour < 22:  # Sunday before 22:00 UTC
        return False
    if weekday == 4 and hour >= 22:  # Friday after 22:00 UTC
        return False
    
    return True


def symbol_to_table(symbol: str) -> str:
    """Convert symbol to table name: BTC/USD -> candles_btc_usd"""
    return f"candles_{symbol.lower().replace('/', '_')}"


# ============================================================================
# BULLETPROOF TICK STREAMER CLASS
# ============================================================================

class BulletproofTickStreamer:
    """
    Production-grade WebSocket tick streamer with:
    - Watchdog timer (kills stale connections)
    - Heartbeat monitoring
    - Aggressive reconnection
    - Health logging
    - Graceful shutdown
    """

    def __init__(self):
        self._batch: List[dict] = []
        self._lock = asyncio.Lock()
        self._shutdown = asyncio.Event()
        self._ws: Optional[websockets.WebSocketClientProtocol] = None
        self._last_tick_time: datetime = datetime.now(timezone.utc)
        self._tick_count: int = 0
        self._connection_count: int = 0
        self._last_health_log: datetime = datetime.now(timezone.utc)
        self._is_connected: bool = False
        self._symbols_streaming: Set[str] = set()
        self._start_time: datetime = datetime.now(timezone.utc)
        
        # REST polling stats
        self._rest_tick_count: int = 0
        self._db_insert_count: int = 0
        self._db_error_count: int = 0

    async def _flush(self):
        """Flush batch to Supabase."""
        async with self._lock:
            if not self._batch:
                return
            payload, self._batch = self._batch, []

        if not sb:
            return

        try:
            sb.table(TICK_TABLE).insert(payload).execute()
            self._db_insert_count += len(payload)
            log.info("‚úÖ Inserted %d ticks into %s", len(payload), TICK_TABLE)
        except Exception as e:
            self._db_error_count += 1
            log.error("‚ùå Insert failed: %s - re-queuing %d rows", e, len(payload))
            async with self._lock:
                self._batch[:0] = payload

    async def _periodic_flush(self):
        """Flush batch every BATCH_FLUSH_SECS."""
        while not self._shutdown.is_set():
            await asyncio.sleep(BATCH_FLUSH_SECS)
            await self._flush()

    async def _handle(self, msg: dict):
        """Handle incoming tick message."""
        if msg.get("event") != "price":
            return

        symbol = msg.get("symbol")
        price = to_float(msg.get("price"))

        if not symbol or price is None:
            return

        self._last_tick_time = datetime.now(timezone.utc)
        self._tick_count += 1
        self._symbols_streaming.add(symbol)

        row = {
            "symbol": symbol,
            "ts": to_timestamp(msg.get("timestamp")).isoformat(),
            "price": price,
            "bid": to_float(msg.get("bid")),
            "ask": to_float(msg.get("ask")),
            "day_volume": (
                to_float(msg.get("day_volume"))
                or to_float(msg.get("dayVolume"))
                or to_float(msg.get("volume"))
            ),
        }

        async with self._lock:
            self._batch.append(row)
            if len(self._batch) >= BATCH_MAX:
                await self._flush()

    async def _watchdog(self):
        """Monitor for stale connections."""
        log.info("üêï Watchdog started (timeout: %ds)", WATCHDOG_TIMEOUT_SECS)

        while not self._shutdown.is_set():
            await asyncio.sleep(5)

            if not self._is_connected:
                continue

            elapsed = (datetime.now(timezone.utc) - self._last_tick_time).total_seconds()

            if elapsed > WATCHDOG_TIMEOUT_SECS:
                log.warning("üêï WATCHDOG: No tick for %.1fs - forcing reconnection!", elapsed)
                if self._ws:
                    with suppress(Exception):
                        await self._ws.close()
                self._is_connected = False

    async def _health_logger(self):
        """Log health status periodically."""
        while not self._shutdown.is_set():
            await asyncio.sleep(HEALTH_LOG_INTERVAL)

            elapsed = (datetime.now(timezone.utc) - self._last_tick_time).total_seconds()
            status = "üü¢ HEALTHY" if elapsed < 10 else "üü° STALE" if elapsed < WATCHDOG_TIMEOUT_SECS else "üî¥ DEAD"
            uptime = (datetime.now(timezone.utc) - self._start_time).total_seconds()

            log.info(
                "üíì HEALTH: %s | WS Ticks: %d | REST Ticks: %d | DB: %d/%d | Streaming: %d | Uptime: %.0fs",
                status,
                self._tick_count,
                self._rest_tick_count,
                self._db_insert_count,
                self._db_error_count,
                len(self._symbols_streaming),
                uptime
            )

    async def _connect_and_stream(self):
        """Single connection attempt with error handling."""
        self._connection_count += 1
        log.info("üîå Connection attempt #%d...", self._connection_count)

        ws_url = f"wss://ws.twelvedata.com/v1/quotes/price?apikey={TD_API_KEY}"

        try:
            self._ws = await asyncio.wait_for(
                websockets.connect(
                    ws_url,
                    ping_interval=20,
                    ping_timeout=20,
                    close_timeout=10,
                    max_queue=2000,
                ),
                timeout=CONNECTION_TIMEOUT_SECS
            )

            await self._ws.send(json.dumps({
                "action": "subscribe",
                "params": {"symbols": WS_SYMBOLS_STR}
            }))

            log.info("üöÄ Connected and subscribed to %d symbols", len(WS_SYMBOLS))
            self._is_connected = True
            self._last_tick_time = datetime.now(timezone.utc)

            async for raw in self._ws:
                if self._shutdown.is_set():
                    break
                try:
                    data = json.loads(raw)
                    
                    if data.get("event") == "subscribe-status":
                        fails = data.get("fails", [])
                        if fails:
                            log.warning("‚ö†Ô∏è Subscribe failures: %d symbols", len(fails))
                        else:
                            log.info("‚úÖ Subscribe confirmed")
                    elif data.get("event") == "heartbeat":
                        self._last_tick_time = datetime.now(timezone.utc)
                    else:
                        await self._handle(data)
                except json.JSONDecodeError:
                    continue

        except asyncio.TimeoutError:
            log.error("‚è±Ô∏è Connection timeout after %ds", CONNECTION_TIMEOUT_SECS)
        except websockets.exceptions.ConnectionClosed as e:
            log.warning("üîå Connection closed: code=%s reason=%s", e.code, e.reason)
        except Exception as e:
            log.error("‚ùå Connection error: %s", e)
        finally:
            self._is_connected = False
            if self._ws:
                with suppress(Exception):
                    await self._ws.close()
            self._ws = None

    async def add_tick(self, tick: dict):
        """Add tick from REST polling."""
        self._rest_tick_count += 1
        async with self._lock:
            self._batch.append(tick)
            if len(self._batch) >= BATCH_MAX:
                await self._flush()

    async def run(self):
        """Main run loop with bulletproof reconnection."""
        log.info("=" * 70)
        log.info("üöÄ BULLETPROOF TICK STREAMER STARTING")
        log.info("=" * 70)
        log.info("üìä WebSocket symbols: %d", len(WS_SYMBOLS))
        log.info("üìà REST symbols: %d", len(STOCK_SYMBOLS))
        log.info("üêï Watchdog timeout: %ds", WATCHDOG_TIMEOUT_SECS)
        log.info("=" * 70)

        flusher = asyncio.create_task(self._periodic_flush())
        watchdog = asyncio.create_task(self._watchdog())
        health = asyncio.create_task(self._health_logger())

        backoff = 1

        try:
            while not self._shutdown.is_set():
                try:
                    await self._connect_and_stream()

                    if self._shutdown.is_set():
                        break

                    log.info("üîÑ Reconnecting in %ds...", backoff)
                    await asyncio.sleep(backoff)
                    backoff = min(MAX_RECONNECT_DELAY, backoff * 2)

                except Exception as e:
                    log.error("üí• Unexpected error: %s", e)
                    await asyncio.sleep(backoff)
                    backoff = min(MAX_RECONNECT_DELAY, backoff * 2)

                if self._tick_count > 0:
                    elapsed = (datetime.now(timezone.utc) - self._last_tick_time).total_seconds()
                    if elapsed < 5:
                        backoff = 1

        finally:
            log.info("üõë Shutting down tick streamer...")
            await self._flush()
            flusher.cancel()
            watchdog.cancel()
            health.cancel()
            with suppress(asyncio.CancelledError):
                await flusher
                await watchdog
                await health

    def shutdown(self):
        """Request graceful shutdown."""
        log.info("üõë Shutdown requested")
        self._shutdown.set()

    def is_shutdown(self) -> bool:
        return self._shutdown.is_set()


# Global streamer instance
tick_streamer = BulletproofTickStreamer()


async def tick_streaming_task():
    """Entry point for tick streaming."""
    await tick_streamer.run()


# ============================================================================
# REST API STOCK POLLING
# ============================================================================

async def fetch_stock_quotes(symbols: List[str]) -> List[Dict]:
    """Fetch quotes for multiple stocks using batch API."""
    if not symbols:
        return []

    results = []

    async with httpx.AsyncClient(timeout=30.0) as client:
        for i in range(0, len(symbols), 120):
            batch = symbols[i:i+120]
            symbols_str = ",".join(batch)

            try:
                url = "https://api.twelvedata.com/quote"
                params = {
                    "symbol": symbols_str,
                    "apikey": TD_API_KEY,
                }

                response = await client.get(url, params=params)
                response.raise_for_status()
                data = response.json()

                if isinstance(data, dict):
                    if "symbol" in data:
                        results.append(data)
                    elif len(batch) > 1:
                        for sym in batch:
                            if sym in data and isinstance(data[sym], dict):
                                results.append(data[sym])

            except Exception as e:
                log.error(f"‚ùå REST API error: {e}")

            if i + 120 < len(symbols):
                await asyncio.sleep(0.5)

    return results


async def stock_polling_task():
    """Poll US stocks via REST API during market hours."""
    log.info(f"üìà Stock polling started ({len(STOCK_SYMBOLS)} symbols)")

    while not tick_streamer.is_shutdown():
        try:
            if not is_us_market_open():
                log.debug("üåô US market closed, sleeping 5 minutes...")
                await asyncio.sleep(300)
                continue

            log.debug(f"üìä Fetching {len(STOCK_SYMBOLS)} stock quotes...")
            quotes = await fetch_stock_quotes(STOCK_SYMBOLS)
            now = datetime.now(timezone.utc)

            for quote in quotes:
                if quote.get("code"):
                    continue

                symbol = quote.get("symbol")
                price = to_float(quote.get("close") or quote.get("price"))

                if not symbol or price is None:
                    continue

                tick = {
                    "symbol": symbol,
                    "ts": now.isoformat(),
                    "price": price,
                    "bid": to_float(quote.get("bid")),
                    "ask": to_float(quote.get("ask")),
                    "day_volume": to_float(quote.get("volume")),
                }

                await tick_streamer.add_tick(tick)

            log.info(f"üìä Fetched {len(quotes)} stock quotes")
            await asyncio.sleep(STOCK_POLL_INTERVAL)

        except Exception as e:
            log.error(f"‚ùå Stock polling error: {e}")
            await asyncio.sleep(30)


# ============================================================================
# ECONOMIC CALENDAR SCRAPER
# ============================================================================

def scrape_trading_economics():
    """Scrape economic calendar from Trading Economics."""
    try:
        url = "https://tradingeconomics.com/calendar"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        log.info("üìÖ Fetching Trading Economics calendar...")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        table = soup.find('table', {'id': 'calendar'})
        if not table:
            log.error("‚ùå Could not find calendar table")
            return []

        events = []
        current_date = None
        current_day = None

        for row in table.find_all('tr'):
            thead = row.find_parent('thead')
            if thead and not row.find('td'):
                if 'hidden-head' in thead.get('class', []):
                    continue
                th = row.find('th')
                if th:
                    date_text = th.get_text(strip=True)
                    try:
                        date_obj = datetime.strptime(date_text, "%A %B %d %Y")
                        current_date = date_obj.strftime("%Y-%m-%d")
                        current_day = date_obj.strftime("%A")
                    except:
                        pass
                continue

            cols = row.find_all('td')
            if len(cols) < 7 or not current_date:
                continue

            try:
                time_str = cols[0].get_text(strip=True)
                country = cols[3].get_text(strip=True) if len(cols) > 3 else ""
                event = cols[4].get_text(strip=True) if len(cols) > 4 else ""

                actual_elem = cols[5].find('span', id='actual') if len(cols) > 5 else None
                actual = actual_elem.get_text(strip=True) if actual_elem else ""

                previous_elem = cols[6].find('span', id='previous') if len(cols) > 6 else None
                previous = previous_elem.get_text(strip=True) if previous_elem else ""

                consensus_elem = cols[7].find(id='consensus') if len(cols) > 7 else None
                consensus = consensus_elem.get_text(strip=True) if consensus_elem else ""

                importance = "Medium"
                event_class = cols[0].find('span')
                if event_class and event_class.get('class'):
                    classes = ' '.join(event_class.get('class'))
                    if 'event-3' in classes or 'event-2' in classes:
                        importance = "High"
                    elif 'event-0' in classes:
                        importance = "Low"

                if not country or len(country) != 2:
                    continue
                if not event or len(event) < 3:
                    continue
                if not time_str:
                    continue

                events.append({
                    'date': current_date,
                    'day': current_day,
                    'time': time_str,
                    'country': country.upper(),
                    'event': event,
                    'actual': actual if actual else None,
                    'forecast': consensus if consensus else None,
                    'previous': previous if previous else None,
                    'consensus': consensus if consensus else None,
                    'importance': importance
                })

            except Exception:
                continue

        log.info(f"üìã Scraped {len(events)} calendar events")
        return events

    except Exception as e:
        log.error(f"‚ùå Error scraping Trading Economics: {e}")
        return []


def upsert_economic_events(events):
    """Insert or update economic events."""
    if not events or not sb:
        return

    try:
        for event in events:
            try:
                existing = sb.table('Economic_calander').select('*').eq(
                    'date', event['date']
                ).eq('time', event['time']).eq(
                    'country', event['country']
                ).eq('event', event['event']).execute()

                if existing.data:
                    sb.table('Economic_calander').update(event).eq(
                        'date', event['date']
                    ).eq('time', event['time']).eq(
                        'country', event['country']
                    ).eq('event', event['event']).execute()
                else:
                    sb.table('Economic_calander').insert(event).execute()
            except Exception:
                continue

        log.info(f"‚úÖ Processed {len(events)} calendar events")
    except Exception as e:
        log.error(f"‚ùå Error upserting events: {e}")


async def economic_calendar_task():
    """Periodically scrape economic calendar."""
    log.info("üìÖ Economic calendar scraper started")

    while not tick_streamer.is_shutdown():
        try:
            loop = asyncio.get_event_loop()
            events = await loop.run_in_executor(None, scrape_trading_economics)

            if events:
                await loop.run_in_executor(None, upsert_economic_events, events)

            await asyncio.sleep(ECON_SCRAPE_INTERVAL)

        except Exception as e:
            log.error(f"‚ùå Calendar task error: {e}")
            await asyncio.sleep(60)


# ============================================================================
# FINANCIAL NEWS SCRAPER
# ============================================================================

RSS_FEEDS = [
    {"url": "http://feeds.bbci.co.uk/news/rss.xml", "name": "BBC News (Main)"},
    {"url": "http://feeds.bbci.co.uk/news/business/rss.xml", "name": "BBC News (Business)"},
    {"url": "http://feeds.bbci.co.uk/news/world/rss.xml", "name": "BBC News (World)"},
    {"url": "https://www.cnbc.com/id/100003114/device/rss/rss.html", "name": "CNBC (Top News)"},
    {"url": "https://www.cnbc.com/id/100727362/device/rss/rss.html", "name": "CNBC (World)"},
    {"url": "https://www.cnbc.com/id/15837362/device/rss/rss.html", "name": "CNBC (US News)"},
    {"url": "http://feeds.marketwatch.com/marketwatch/realtimeheadlines", "name": "MarketWatch (Real-time)"},
    {"url": "http://feeds.marketwatch.com/marketwatch/topstories", "name": "MarketWatch (Top Stories)"},
    {"url": "https://www.coindesk.com/arc/outboundfeeds/rss/", "name": "CoinDesk"},
    {"url": "https://cointelegraph.com/rss", "name": "Cointelegraph"},
    {"url": "https://www.theguardian.com/world/rss", "name": "The Guardian (World)"},
    {"url": "https://www.theguardian.com/uk/business/rss", "name": "The Guardian (Business)"},
    {"url": "https://www.aljazeera.com/xml/rss/all.xml", "name": "Al Jazeera"},
]

INCLUDE_KEYWORDS = [
    "war", "attack", "invasion", "military", "strike", "missile", "conflict", "terrorism", "coup",
    "trump", "biden", "election", "president", "prime minister", "government", "resign",
    "fed", "federal reserve", "powell", "ecb", "lagarde", "bank of england", "rate", "interest",
    "monetary", "qe", "quantitative", "inflation", "cpi", "ppi", "gdp", "unemployment", "jobs",
    "nfp", "payroll", "retail sales", "economic", "recession", "tariff", "sanction", "trade war",
    "import", "export", "ban", "bitcoin", "btc", "ethereum", "eth", "crypto", "cryptocurrency",
    "blockchain", "crash", "plunge", "surge", "rally", "collapse", "soar", "tumble", "selloff",
    "oil", "opec", "gold", "crude", "brent", "wti", "commodity", "bank", "banking", "bailout",
    "liquidity", "credit", "earthquake", "hurricane", "disaster", "emergency", "crisis", "pandemic",
    "outbreak",
]

EXCLUDE_KEYWORDS = [
    "sport", "football", "soccer", "cricket", "tennis", "basketball", "baseball",
    "celebrity", "actor", "actress", "movie", "film", "tv show", "television",
    "recipe", "cooking", "fashion", "style", "beauty", "wedding", "divorce", "dating",
]


def get_news_scan_interval():
    """Smart scheduling based on market hours."""
    now = datetime.utcnow()
    if now.weekday() >= 5:
        return 14400, "weekend"
    if (8 <= now.hour < 20) or (0 <= now.hour < 1):
        return 900, "active"
    return 1800, "quiet"


def should_prefilter_article(title: str, summary: str) -> bool:
    """Pre-filter articles by keywords."""
    text = f"{title} {summary}".lower()

    for exclude_word in EXCLUDE_KEYWORDS:
        if exclude_word in text:
            if "royal bank" in text:
                continue
            return False

    for include_word in INCLUDE_KEYWORDS:
        if include_word in text:
            return True

    return False


def calculate_title_similarity(title1: str, title2: str) -> float:
    """Calculate similarity between two titles."""
    return SequenceMatcher(None, title1.lower(), title2.lower()).ratio()


def optimize_articles_for_cost(articles: list) -> list:
    """Optimize articles to reduce API costs."""
    if not articles:
        return []

    log.info(f"üí∞ Cost optimization: Starting with {len(articles)} articles")

    # Filter: must have images
    articles_with_images = [a for a in articles if a.get('image_url')]
    log.info(f"   Removed {len(articles) - len(articles_with_images)} without images")

    # Deduplicate similar titles
    unique_articles = []
    seen_topics = []

    for article in articles_with_images:
        is_duplicate = False
        for seen_title in seen_topics:
            if calculate_title_similarity(article['title'], seen_title) >= 0.75:
                is_duplicate = True
                break

        if not is_duplicate:
            unique_articles.append(article)
            seen_topics.append(article['title'])

    log.info(f"   Removed {len(articles_with_images) - len(unique_articles)} similar articles")

    # Category limits
    category_counts = {'crypto': 0, 'political': 0, 'central_bank': 0, 'market': 0}
    category_limits = {'crypto': 2, 'political': 2, 'central_bank': 2, 'market': 2}

    crypto_kw = ['bitcoin', 'btc', 'ethereum', 'eth', 'crypto', 'solana', 'xrp']
    political_kw = ['trump', 'biden', 'election', 'president', 'white house']
    central_bank_kw = ['fed', 'powell', 'ecb', 'lagarde', 'rate', 'monetary']
    market_kw = ['stock', 'nasdaq', 's&p', 'dow', 'market']

    final_articles = []
    for article in unique_articles:
        text = f"{article['title']} {article['summary']}".lower()

        category = None
        if any(kw in text for kw in crypto_kw):
            category = 'crypto'
        elif any(kw in text for kw in political_kw):
            category = 'political'
        elif any(kw in text for kw in central_bank_kw):
            category = 'central_bank'
        elif any(kw in text for kw in market_kw):
            category = 'market'

        if category and category in category_limits:
            if category_counts[category] >= category_limits[category]:
                continue
            category_counts[category] += 1

        final_articles.append(article)

    savings_pct = int(((len(articles) - len(final_articles)) / len(articles)) * 100) if articles else 0
    log.info(f"   üí∞ Cost savings: {savings_pct}% | Final: {len(final_articles)} articles")

    return final_articles


def fetch_rss_feed(feed_url: str, source_name: str) -> list:
    """Fetch and parse RSS feed."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; FinancialNewsBot/1.0)'}
        response = requests.get(feed_url, headers=headers, timeout=15)
        response.raise_for_status()

        feed = feedparser.parse(response.content)
        articles = []
        cutoff_date = datetime.now() - timedelta(hours=24)

        for entry in feed.entries:
            pub_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_date = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                pub_date = datetime(*entry.updated_parsed[:6])

            if pub_date and pub_date < cutoff_date:
                continue

            title = entry.get('title', '').strip()
            url = entry.get('link', '').strip()
            summary = entry.get('summary', entry.get('description', '')).strip()[:500]

            if not title or not url:
                continue

            image_url = None
            if hasattr(entry, 'media_content') and entry.media_content:
                image_url = entry.media_content[0].get('url')
            if not image_url and hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                image_url = entry.media_thumbnail[0].get('url')
            if not image_url and hasattr(entry, 'enclosures') and entry.enclosures:
                for enclosure in entry.enclosures:
                    if enclosure.get('type', '').startswith('image/'):
                        image_url = enclosure.get('href') or enclosure.get('url')
                        break

            articles.append({
                'title': title,
                'url': url,
                'source': source_name,
                'published': pub_date.isoformat() if pub_date else None,
                'summary': summary,
                'image_url': image_url
            })

        return articles

    except Exception as e:
        log.warning(f"‚ö†Ô∏è RSS error ({source_name}): {e}")
        return []


def classify_article_with_deepseek(article: dict) -> dict:
    """Classify article using DeepSeek API."""
    if not deepseek_client:
        return None

    try:
        prompt = f"""You are a professional financial news analyst. Analyze this article.

TITLE: {article['title']}
SUMMARY: {article['summary']}
SOURCE: {article['source']}

Respond with ONLY valid JSON:
{{"is_important": true, "event_type": "war", "impact_level": "high", "affected_symbols": ["BTC/USD", "XAU/USD"], "sentiment": "bearish", "summary": "2-3 sentence market impact", "keywords": ["keyword1"]}}

Valid event_types: war, political_shock, economic_data, central_bank, trade_war, commodity_shock, market_crash, crypto_crash, crypto_rally, natural_disaster, bank_crisis, energy_crisis, pandemic_health, market_rally
Valid impact_levels: critical, high, medium, low
Valid sentiments: bullish, bearish, neutral"""

        response = deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.1
        )

        response_text = response.choices[0].message.content.strip()

        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
            response_text = response_text.strip()
        if response_text.endswith("```"):
            response_text = response_text[:-3].strip()

        return json.loads(response_text)

    except Exception as e:
        log.warning(f"‚ö†Ô∏è DeepSeek error: {e}")
        return None


def store_news_article(article: dict, classification: dict) -> bool:
    """Store classified news article."""
    if not sb:
        return False

    try:
        data = {
            'title': article['title'],
            'summary': classification.get('summary', article['summary'][:200]),
            'url': article['url'],
            'source': article['source'],
            'published_at': article['published'],
            'image_url': article.get('image_url'),
            'event_type': classification.get('event_type'),
            'impact_level': classification.get('impact_level'),
            'affected_symbols': classification.get('affected_symbols', []),
            'sentiment': classification.get('sentiment'),
            'keywords': classification.get('keywords', []),
            'is_breaking': classification.get('impact_level') in ['critical', 'high']
        }

        sb.table('financial_news').insert(data).execute()
        return True

    except Exception as e:
        if 'duplicate' not in str(e).lower():
            log.warning(f"‚ö†Ô∏è News store error: {e}")
        return False


async def financial_news_task():
    """Financial news scraper with smart scheduling."""
    log.info("üì∞ Financial news scraper started")

    while not tick_streamer.is_shutdown():
        try:
            interval, session_name = get_news_scan_interval()
            session_emoji = "üî•" if session_name == "active" else "üò¥" if session_name == "quiet" else "üåô"

            log.info(f"üîç News scan ({session_emoji} {session_name} session)")

            all_articles = []
            loop = asyncio.get_event_loop()

            for feed in RSS_FEEDS:
                articles = await loop.run_in_executor(None, fetch_rss_feed, feed['url'], feed['name'])
                all_articles.extend(articles)
                await asyncio.sleep(0.5)

            log.info(f"üìã Fetched {len(all_articles)} total articles")

            # Deduplicate by URL
            seen_urls = set()
            unique_articles = [a for a in all_articles if not (a['url'] in seen_urls or seen_urls.add(a['url']))]

            # Pre-filter
            filtered = [a for a in unique_articles if should_prefilter_article(a['title'], a['summary'])]
            log.info(f"üéØ After filtering: {len(filtered)} articles")

            # Cost optimization
            filtered = optimize_articles_for_cost(filtered)

            stored_count = 0
            for article in filtered:
                classification = await loop.run_in_executor(None, classify_article_with_deepseek, article)

                if not classification:
                    continue

                if not classification.get('is_important') or classification.get('impact_level') not in ['critical', 'high']:
                    continue

                impact_emoji = "üö®" if classification['impact_level'] == 'critical' else "‚ö°"
                log.info(f"{impact_emoji} {article['title'][:60]}...")

                if await loop.run_in_executor(None, store_news_article, article, classification):
                    stored_count += 1

                await asyncio.sleep(1)

            log.info(f"‚úÖ News cycle: {stored_count} stored")
            log.info(f"üò¥ Next scan in {interval // 60} minutes")
            await asyncio.sleep(interval)

        except Exception as e:
            log.error(f"‚ùå News task error: {e}")
            await asyncio.sleep(60)


# ============================================================================
# GAP DETECTION AND BACKFILL
# ============================================================================

TD_REST_URL = "https://api.twelvedata.com/time_series"


def detect_gaps(symbol: str, lookback_hours: int = MAX_GAP_HOURS) -> List[tuple]:
    """Detect gaps in candle data."""
    if not sb:
        return []
        
    table_name = symbol_to_table(symbol)

    try:
        since = (datetime.now(timezone.utc) - timedelta(hours=lookback_hours)).isoformat()

        result = sb.table(table_name).select('timestamp').gte(
            'timestamp', since
        ).order('timestamp', desc=False).execute()

        if not result.data or len(result.data) < 2:
            return []

        gaps = []
        timestamps = [datetime.fromisoformat(r['timestamp'].replace('Z', '+00:00')) for r in result.data]

        for i in range(1, len(timestamps)):
            prev_ts = timestamps[i - 1]
            curr_ts = timestamps[i]
            gap_minutes = (curr_ts - prev_ts).total_seconds() / 60

            if gap_minutes > MIN_GAP_MINUTES:
                gap_start = prev_ts + timedelta(minutes=1)
                gap_end = curr_ts - timedelta(minutes=1)
                gaps.append((gap_start, gap_end))

        return gaps

    except Exception as e:
        log.error(f"‚ùå Gap detection error for {symbol}: {e}")
        return []


def fetch_candles_rest(symbol: str, start_date: datetime, end_date: datetime) -> List[Dict]:
    """Fetch historical candles from TwelveData REST API."""
    try:
        params = {
            "symbol": symbol,
            "interval": "1min",
            "start_date": start_date.strftime("%Y-%m-%d %H:%M:%S"),
            "end_date": end_date.strftime("%Y-%m-%d %H:%M:%S"),
            "apikey": TD_API_KEY,
            "format": "JSON",
            "timezone": "UTC"
        }

        response = requests.get(TD_REST_URL, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if "values" not in data:
            return []

        candles = []
        for v in data["values"]:
            try:
                candles.append({
                    "timestamp": datetime.strptime(v["datetime"], "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc),
                    "open": float(Decimal(str(v["open"]))),
                    "high": float(Decimal(str(v["high"]))),
                    "low": float(Decimal(str(v["low"]))),
                    "close": float(Decimal(str(v["close"]))),
                    "symbol": symbol
                })
            except Exception:
                continue

        return candles

    except Exception as e:
        log.error(f"‚ùå Candle fetch error for {symbol}: {e}")
        return []


def insert_candles(symbol: str, candles: List[Dict]) -> int:
    """Insert candles using upsert."""
    if not candles or not sb:
        return 0

    table_name = symbol_to_table(symbol)

    try:
        rows = [
            {
                "timestamp": c["timestamp"].isoformat(),
                "open": c["open"],
                "high": c["high"],
                "low": c["low"],
                "close": c["close"],
                "symbol": c["symbol"]
            }
            for c in candles
        ]

        sb.table(table_name).upsert(rows, on_conflict="timestamp").execute()
        return len(rows)

    except Exception as e:
        log.error(f"‚ùå Candle insert error for {symbol}: {e}")
        return 0


async def fill_gaps_for_symbol(symbol: str, gaps: List[tuple]) -> int:
    """Fill all detected gaps for a symbol."""
    total_filled = 0

    for gap_start, gap_end in gaps:
        gap_minutes = int((gap_end - gap_start).total_seconds() / 60) + 1
        log.info(f"üîß Filling {symbol}: {gap_start.strftime('%H:%M')}-{gap_end.strftime('%H:%M')} ({gap_minutes}min)")

        loop = asyncio.get_event_loop()
        candles = await loop.run_in_executor(None, fetch_candles_rest, symbol, gap_start, gap_end)

        if candles:
            inserted = await loop.run_in_executor(None, insert_candles, symbol, candles)
            total_filled += inserted
            log.info(f"‚úÖ Filled {inserted} candles for {symbol}")

        await asyncio.sleep(BACKFILL_RATE_LIMIT)

    return total_filled


async def startup_backfill(hours: int = 2):
    """On startup, backfill last N hours."""
    log.info(f"üöÄ Running startup backfill for last {hours} hours...")

    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(hours=hours)

    total_candles = 0
    loop = asyncio.get_event_loop()

    for symbol in CANDLE_SYMBOLS:
        log.info(f"‚¨áÔ∏è Backfilling {symbol}...")

        candles = await loop.run_in_executor(None, fetch_candles_rest, symbol, start_time, end_time)

        if candles:
            inserted = await loop.run_in_executor(None, insert_candles, symbol, candles)
            total_candles += inserted
            log.info(f"‚úÖ {symbol}: {inserted} candles")

        await asyncio.sleep(BACKFILL_RATE_LIMIT)

    log.info(f"üéâ Startup backfill complete: {total_candles} total candles")


async def gap_fill_task():
    """Periodic gap detection and filling."""
    log.info("üîß Gap fill service started")
    log.info(f"   Scan interval: {GAP_SCAN_INTERVAL}s | Lookback: {MAX_GAP_HOURS}h")

    # Run startup backfill first
    await startup_backfill(hours=2)

    while not tick_streamer.is_shutdown():
        try:
            log.info("üîç Scanning for gaps...")

            total_gaps = 0
            total_filled = 0

            for symbol in CANDLE_SYMBOLS:
                gaps = detect_gaps(symbol)

                if gaps:
                    total_gaps += len(gaps)
                    filled = await fill_gaps_for_symbol(symbol, gaps)
                    total_filled += filled
                    await asyncio.sleep(BACKFILL_RATE_LIMIT)

            if total_gaps > 0:
                log.info(f"üìä Gap scan: {total_gaps} gaps found, {total_filled} candles filled")
            else:
                log.info("‚úÖ No gaps detected")

            await asyncio.sleep(GAP_SCAN_INTERVAL)

        except Exception as e:
            log.error(f"‚ùå Gap fill error: {e}")
            await asyncio.sleep(60)


# ============================================================================
# FRED MACRO DATA
# ============================================================================

def fetch_fred_series(series_id: str, observation_start: str, observation_end: str) -> List[Dict]:
    """Fetch FRED data."""
    if not FRED_API_KEY:
        return []

    url = f"{FRED_API_BASE}/series/observations"
    params = {
        'series_id': series_id,
        'api_key': FRED_API_KEY,
        'file_type': 'json',
        'observation_start': observation_start,
        'observation_end': observation_end,
        'sort_order': 'desc'
    }

    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if 'observations' not in data:
            return []

        return [
            {'date': obs['date'], 'value': obs['value']}
            for obs in data['observations']
            if obs['value'] != '.'
        ]

    except Exception as e:
        log.error(f"‚ùå FRED error for {series_id}: {e}")
        return []


def get_active_indicators() -> List[Dict]:
    """Get active macro indicators from metadata."""
    if not sb:
        return []
    try:
        result = sb.table('macro_indicator_metadata').select('*').eq('is_active', True).execute()
        return result.data if result.data else []
    except Exception:
        return []


async def macro_data_task():
    """Event-driven macro data updater."""
    log.info("üìä Macro data task started")

    while not tick_streamer.is_shutdown():
        try:
            log.info("üîç Checking for macro data updates...")
            # Placeholder - implement full FRED logic as needed
            await asyncio.sleep(3600)
        except Exception as e:
            log.error(f"‚ùå Macro data error: {e}")
            await asyncio.sleep(300)


# ============================================================================
# HEALTH CHECK SERVER
# ============================================================================

class HealthHandler(BaseHTTPRequestHandler):
    """HTTP health check handler."""

    def log_message(self, format, *args):
        pass

    def do_GET(self):
        if self.path in ('/', '/health'):
            uptime = (datetime.now(timezone.utc) - tick_streamer._start_time).total_seconds()

            health_data = {
                "status": "healthy",
                "uptime_seconds": int(uptime),
                "websocket_connected": tick_streamer._is_connected,
                "ws_ticks": tick_streamer._tick_count,
                "rest_ticks": tick_streamer._rest_tick_count,
                "db_inserts": tick_streamer._db_insert_count,
                "db_errors": tick_streamer._db_error_count,
                "symbols_streaming": len(tick_streamer._symbols_streaming),
            }

            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(health_data).encode())
        else:
            self.send_response(404)
            self.end_headers()


def run_health_server():
    """Run health check HTTP server."""
    server = HTTPServer(('0.0.0.0', PORT), HealthHandler)
    log.info(f"üè• Health server listening on port {PORT}")
    server.serve_forever()


# ============================================================================
# MAIN
# ============================================================================

async def main():
    """Run all tasks in parallel."""
    log.info("=" * 70)
    log.info("üèõÔ∏è LONDON STRATEGIC EDGE - COMPLETE WORKER v3.0")
    log.info("=" * 70)
    log.info("   1Ô∏è‚É£ Tick streaming (WebSocket: %d + REST: %d = %d symbols)",
             len(WS_SYMBOLS), len(STOCK_SYMBOLS), len(WS_SYMBOLS) + len(STOCK_SYMBOLS))
    log.info("   2Ô∏è‚É£ Economic calendar (Trading Economics)")
    log.info("   3Ô∏è‚É£ Financial news (%d RSS feeds + DeepSeek)", len(RSS_FEEDS))
    log.info("   4Ô∏è‚É£ Gap detection & backfill (AUTO-HEALING)")
    log.info("   5Ô∏è‚É£ Macro data (FRED API)")
    log.info("   6Ô∏è‚É£ Health server (port %d)", PORT)
    log.info("=" * 70)

    # Start health server in background
    health_thread = threading.Thread(target=run_health_server, daemon=True)
    health_thread.start()

    # Setup graceful shutdown
    def signal_handler(sig, frame):
        log.info(f"üõë Received signal {sig}, initiating shutdown...")
        tick_streamer.shutdown()

    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    # Create tasks
    tasks = [
        asyncio.create_task(tick_streaming_task(), name="tick_stream"),
        asyncio.create_task(stock_polling_task(), name="stock_poll"),
        asyncio.create_task(economic_calendar_task(), name="econ_calendar"),
        asyncio.create_task(financial_news_task(), name="news"),
        asyncio.create_task(gap_fill_task(), name="gap_fill"),
        asyncio.create_task(macro_data_task(), name="macro"),
    ]

    try:
        await asyncio.gather(*tasks, return_exceptions=True)
    except Exception as e:
        log.error(f"‚ùå Main loop error: {e}")
    finally:
        tick_streamer.shutdown()
        for task in tasks:
            task.cancel()
            with suppress(asyncio.CancelledError):
                await task
        log.info("üëã Worker shutdown complete")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("üëã Goodbye!")
        sys.exit(0)
